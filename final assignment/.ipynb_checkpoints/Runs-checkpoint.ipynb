{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import (Model, RealParameter, Policy, Constant, Scenario, Constraint, \n",
    "                           ScalarOutcome, MultiprocessingEvaluator, ema_logging, perform_experiments)\n",
    "from ema_workbench.util import ema_logging, save_results, load_results\n",
    "from ema_workbench.em_framework.optimization import (EpsilonProgress, HyperVolume, ArchiveLogger)\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "from ema_workbench.em_framework import sample_uncertainties\n",
    "from ema_workbench.em_framework.evaluators import BaseEvaluator\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "dike_model, planning_steps = get_model_for_problem_formulation(2) # assign problem_formulation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 \n",
    "import numpy.random\n",
    "import random\n",
    "numpy.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import functools\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# percentile90 = functools.partial(np.percentile, q=90)\n",
    "# def var_mean(data):\n",
    "#     return ((np.percentile(data,q=75)-np.percentile(data,q=25))*np.sum(data)/(data.shape[0]))\n",
    "\n",
    "# The average over the scenarios in billions\n",
    "\n",
    "def mean_rob(data):\n",
    "    return np.sum(data)/(data.shape[0]*1e9)\n",
    "\n",
    "\n",
    "\n",
    "def threshold(direction, threshold, data):\n",
    "    if direction == SMALLER:\n",
    "        return np.sum(data<=threshold)/data.shape[0]\n",
    "    else:\n",
    "        return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "death_funcs = functools.partial(threshold, \"SMALLER\", 0.00001)\n",
    "damage_funcs = functools.partial(threshold, \"SMALLER\", 100000)\n",
    "\n",
    "robustness_functions = [ScalarOutcome('Robustness metric Damage', kind=ScalarOutcome.MAXIMIZE,\n",
    "                             variable_name='Expected Annual Damage', function = damage_funcs),\n",
    "                        ScalarOutcome('Robustness metric Dike Costs', kind=ScalarOutcome.MINIMIZE,\n",
    "                             variable_name='Dike Investment Costs', function = mean_rob),\n",
    "                        ScalarOutcome('Robustness metric RfR Investement Costs', kind=ScalarOutcome.MINIMIZE,\n",
    "                             variable_name='RfR Investment Costs', function = mean_rob),\n",
    "                        ScalarOutcome('Robustness metric Evacuation Costs', kind=ScalarOutcome.MINIMIZE,\n",
    "                             variable_name='Evacuation Costs', function = mean_rob),\n",
    "                        ScalarOutcome('Robustness metric Deaths', kind=ScalarOutcome.MAXIMIZE,\n",
    "                             variable_name='Expected Number of Deaths', function = death_funcs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Annual Damage\n",
    "# RfR Investment Costs\n",
    "# Evacuation Costs\n",
    "# Expected Number of Deaths\n",
    "# Total Investment Costs\n",
    "scen = 150\n",
    "pol = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scen = 150\n",
    "pol = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] performing 150 scenarios * 12 policies * 1 model(s) = 1800 experiments\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from ema_workbench import ema_logging, MultiprocessingEvaluator\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=scen,               \n",
    "                                            policies=pol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results, f\"Outputs/perform_experiments/scen{scen}pol{pol}.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] performing 10 scenarios * 10 policies * 1 model(s) = 100 experiments\n",
      "[MainProcess/INFO] 10 cases completed\n",
      "[MainProcess/INFO] 20 cases completed\n",
      "[MainProcess/INFO] 30 cases completed\n",
      "[MainProcess/INFO] 40 cases completed\n",
      "[MainProcess/INFO] 50 cases completed\n",
      "[MainProcess/INFO] 60 cases completed\n",
      "[MainProcess/INFO] 70 cases completed\n",
      "[MainProcess/INFO] 80 cases completed\n",
      "[MainProcess/INFO] 90 cases completed\n",
      "[MainProcess/INFO] 100 cases completed\n",
      "[MainProcess/INFO] experiments finished\n",
      "[MainProcess/INFO] terminating pool\n",
      "[MainProcess/INFO] results saved successfully to C:\\Users\\georg\\OneDrive\\Documents\\GitHub\\MBDM-group-11\\final assignment\\results\\open_exploration_uncertainties_10000runs.tar.gz\n",
      "[MainProcess/INFO] results loaded succesfully from C:\\Users\\georg\\OneDrive\\Documents\\GitHub\\MBDM-group-11\\final assignment\\results\\open_exploration_uncertainties_10000runs.tar.gz\n",
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] performing 10 scenarios * 1 policies * 1 model(s) = 10 experiments\n",
      "[MainProcess/INFO] 1 cases completed\n",
      "[MainProcess/INFO] 2 cases completed\n",
      "[MainProcess/INFO] 3 cases completed\n",
      "[MainProcess/INFO] 4 cases completed\n",
      "[MainProcess/INFO] 5 cases completed\n",
      "[MainProcess/INFO] 6 cases completed\n",
      "[MainProcess/INFO] 7 cases completed\n",
      "[MainProcess/INFO] 8 cases completed\n",
      "[MainProcess/INFO] 9 cases completed\n",
      "[MainProcess/INFO] 10 cases completed\n",
      "[MainProcess/INFO] experiments finished\n",
      "[MainProcess/INFO] terminating pool\n",
      "[MainProcess/INFO] results saved successfully to C:\\Users\\georg\\OneDrive\\Documents\\GitHub\\MBDM-group-11\\final assignment\\results\\open_exploration_uncertainties_policy0.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ema_workbench import (MultiprocessingEvaluator,\n",
    "                           Scenario, Constraint, Policy,\n",
    "                           ScalarOutcome)\n",
    "from ema_workbench.em_framework.optimization import EpsilonProgress, HyperVolume\n",
    "from ema_workbench.em_framework.evaluators import perform_experiments, optimize\n",
    "from ema_workbench.util import ema_logging, save_results, load_results\n",
    "from ema_workbench.analysis import (pairs_plotting, prim, \n",
    "                                    feature_scoring, parcoords,\n",
    "                                    dimensional_stacking, plotting)\n",
    "\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "\n",
    "'''PROBLEM FORMULATION\n",
    "   ===================\n",
    "\n",
    "   For different list of outcomes:\n",
    "    0 = 2-objective PF\n",
    "    1 = 3-objective PF\n",
    "    2 = 5-objective PF\n",
    "    3 = Disaggregate over locations\n",
    "    4 = Disaggregate over time\n",
    "    5 = Fully disaggregated\n",
    "'''\n",
    "\n",
    "'''PERFORM EXPERIMENTS : RANDOM POLICIES\n",
    "   =====================================\n",
    "   \n",
    "   For open exploration over uncertainties, \n",
    "   perform expriments for ten sampled polices\n",
    "   over 1,000 sampled uncertainties\n",
    "'''\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios= 1000, policies = 10) # using lhs sampling\n",
    "\n",
    "save_results(results, './results/open_exploration_uncertainties_10000runs.tar.gz')\n",
    "\n",
    "results = load_results('./results/open_exploration_uncertainties_10000runs.tar.gz')\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "'''PERFORM EXPRIMENTS : ZERO POLICY\n",
    "   ================================\n",
    "\n",
    "   Perform experiments for zero policy \n",
    "   using the same sampled scenarios\n",
    "\n",
    "'''\n",
    "\n",
    "# use the same 1,000 sampled scenarios, run for zero policy\n",
    "sampled_scenarios = experiments.loc[:, [u.name for u in dike_model.uncertainties]]\n",
    "sampled_scenarios.drop_duplicates(inplace=True)\n",
    "ref_scenarios = [Scenario(i, **scenario.to_dict())\n",
    "                 for i, scenario in pd.DataFrame.from_dict(sampled_scenarios).iterrows()] # sampled scenarios\n",
    "\n",
    "policy0 = {'DikeIncrease': 0, 'DaysToThreat': 0, 'RfR': 0}\n",
    "ref_policy = {}\n",
    "for key in dike_model.levers:\n",
    "    _, s = key.name.split('_')\n",
    "    if ' ' in s:\n",
    "        s, _ = s.split(' ')\n",
    "    ref_policy.update({key.name: policy0[s]})                \n",
    "\n",
    "ref_policy0 = Policy('baseline', **ref_policy) # policy0\n",
    "\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios= ref_scenarios, policies = ref_policy0) # using lhs sampling\n",
    "\n",
    "save_results(results, './results/open_exploration_uncertainties_policy0.tar.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
